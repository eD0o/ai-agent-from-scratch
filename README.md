# 2 - LLM Overview

## 2.1 - Overview

### 2.1.1 - What is an LLM?

#### Definition

- A Large Language Model (LLM) is a `deep learning model trained on massive amounts of text`.
- It learns to `predict the next most likely token in a sequence using statistical patterns` in language.
- Despite their impressive output, `LLMs are fundamentally next-token predictors` ‚Äî not sentient or conscious.

#### ü§ñ Key Characteristics

- Built on the Transformer architecture (from the ‚ÄúAttention Is All You Need‚Äù paper, 2017).
- Trained on hundreds of billions of tokens.
- Uses attention mechanisms to track context and relationships.
- Capable of `few-shot, zero-shot, and task-agnostic` language generation.

### 2.1.2 - How LLMs Work

#### üî¢ Tokens

- LLMs operate on tokens, which are usually 3‚Äì4 characters long.

- Before processing:

  1. Text is tokenized.
  2. Tokens are embedded into vectors (e.g., 1536-dimensional).
  3. Vectors go through multiple transformer layers.

- Why this matters:

  - `Tokens determine cost, latency, and context length`.
    > Exceeding the model‚Äôs context window results in truncation or errors.

#### üß† Pattern Recognition

- Through training, LLMs learn:

  - Grammar & syntax
  - Logic & reasoning patterns
  - Domain-specific language
  - Statistical correlations (sometimes hallucinations)

#### üß± Transformer Architecture (Core Components)

- Self-attention layers ‚Äî> weigh importance of tokens relative to others
- Feed-forward layers ‚Äî> build complex representations
- Layer normalization ‚Äî> stabilize training
- Positional encodings ‚Äî> provide order awareness

### 2.1.3 - Training LLMs

#### ‚öôÔ∏è Pretraining Phase

- Trained with a next-token prediction objective.
- Trained on broad internet-scale datasets (textbooks, forums, code, etc.).
- Uses gradient descent to optimize prediction accuracy.

#### üõ†Ô∏è Fine-Tuning Phase (Optional)

- Instruction tuning: teaches the model to follow instructions better.
- RLHF (Reinforcement Learning from Human Feedback):

  - Humans rank outputs.
  - A reward model is trained to reflect those preferences.
  - The LLM is fine-tuned to maximize that reward.

### 2.1.4 - LLMs in Practice

#### üì¶ Model Outputs

- You input a prompt ‚Üí Model generates probabilities for next tokens ‚Üí It samples one ‚Üí Repeats until completion.
- `All output is based on token probability distributions`, not facts or external understanding.

#### üí∞ Token Costs & Limits

- LLM APIs (like OpenAI or Anthropic) `charge per token, both input and output`.
- Each model has a context window:

  - Smaller (2K‚Äì8K tokens) = faster, cheaper
  - Larger (32K‚Äì128K+) = handles long documents, but more expensive

### 2.1.5 - Key Takeaways

- `LLMs are powerful prediction engines`, not databases or calculators.
- `Transformers and tokens are at the heart` of everything.
- Understanding `vectorization, attention, and token limits` gives you a massive edge when building with LLMs.
- You don‚Äôt need to train your own ‚Äî but `you do need to understand how they work to use them well`.




## Additional Resources

- ["Attention Is All You Need" paper (original Transformer paper)](https://arxiv.org/abs/1706.03762)
- [Hugging Face documentation](https://huggingface.co/docs)
- [OpenAI API documentation](https://platform.openai.com/docs/api-reference/introduction)
- [Anthropic Claude documentation](https://docs.anthropic.com/en/docs/welcome)